<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>app.dti.proto.DTILCSTest API documentation</title>
<meta name="description" content="This software project was created in 2023 by the U.S. Federal government.
See INTENT.md for information about what that means. See CONTRIBUTORS.md
â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>app.dti.proto.DTILCSTest</code></h1>
</header>
<section id="section-intro">
<p>This software project was created in 2023 by the U.S. Federal government.
See INTENT.md for information about what that means. See CONTRIBUTORS.md and
LICENSE.md for licensing, copyright, and attribution information.</p>
<p>Copyright 2023 U.S. Federal Government (in countries where recognized)
Copyright 2023 Michael Todd and Gilbert Peterson</p>
<p>Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<p>This package is an extention of the scikit-ExSTraCS project located at
<a href="https://github.com/UrbsLab/scikit-ExSTraCS.git.">https://github.com/UrbsLab/scikit-ExSTraCS.git.</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This software project was created in 2023 by the U.S. Federal government.
See INTENT.md for information about what that means. See CONTRIBUTORS.md and
LICENSE.md for licensing, copyright, and attribution information.

Copyright 2023 U.S. Federal Government (in countries where recognized)
Copyright 2023 Michael Todd and Gilbert Peterson

Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

This package is an extention of the scikit-ExSTraCS project located at 
https://github.com/UrbsLab/scikit-ExSTraCS.git.
&#34;&#34;&#34;
import math
import os
import pathlib
import numpy as np
from app.dti.proto.Utils import Timer
from app.dti.proto.DTILCS import DTILCS

class DTILCSTest:
    &#34;&#34;&#34;Digital Trace Inspector Learning Classifier System Test Suite is a custom test parameter testings suite to provide
    an interface for users of DTILCS to build many models and adjust hyperparameters. This suite also creats reports 
    showing results from each trial.
    &#34;&#34;&#34;
    def __int__(self, iters=[10000], n_vals=[6000], nu_vals=[5])-&gt;None:
        self.dti = None
        self.tests = 5
        self.cv_fold = 3
        self.n_vals = n_vals
        self.nu_vals = nu_vals
        self.iters = iters
        self.training_data_file = None
        self.test_data_file = None
        self.cv = []
        self.accuracy = []
        self.bal_accuracy = []
        self.error = []
        self.true_neg = []
        self.false_pos = []
        self.false_neg = []
        self.true_pos = []
        self.total_obs = []
        self.fit_times = []
        self.class_label = None
        self.rules_dir = None

    def compile(
        self,
        training_data_file=None,
        test_data_file=None,
        test_name=None,
        do_ek_scores=False,
        report_dir=&#34;./reports/&#34;,
        tests=10,
        cv_fold=3,
        iters=[10000],
        n_vals=[6000],
        nu_vals=[5],
        params=None,
    ):
        &#34;&#34;&#34;Compile test settings before executing.

        Args:
            training_data_file (_type_, optional): Feature vector used to train the model. Defaults to None.
            test_data_file (_type_, optional): Feature vector to test. Defaults to None.
            class_label (str, optional): Class label used in feature vector. Defaults to &#34;relevant&#34;.
            test_name (str, optional): Title of the test for saving results. Defaults to &#34;Test&#34;.
            do_ek_scores (bool, optional): Fit option, if true use skrebate.ReliefF to calculate feature importance and speedup training. Defaults to False.
            report_dir (str, optional): Output directory for results. Defaults to &#34;./reports/&#34;.
            tests (int, optional): Number of tests to run at each setting level. Defaults to 10.
            cv_fold (int, optional): Cross validation fold value. Defaults to 3.
            iters (list, optional): Training iterations (epochs) through training dataset. Defaults to [10000].
            n_vals (list, optional): N-value hyper parameter -- maximum micro classifier population size (sum of classifier numerosities). Defaults to [6000].
            nu_vals (list, optional): Power parameter used to determine the importance of high accuracy when calculating fitness. Defaults to [5].
            params (_type_, optional): Configuration dict to configure model with a batch of settings. Defaults to None.

        Raises:
            ValueError: If number of tests not an int &gt;= 1
            ValueError: If number of cv folds not an int &gt;= 2
            ValueError: If number of iters not positve int &gt;= 1
            ValueError: If number of n_vals not positve int &gt;= 1
            ValueError: If nu_vals not positve int &gt;= 1
            ValueError: If training_data_file not a str
            ValueError: If test_data_file not a str
            ValueError: If test_name value not a str
            ValueError: If test_name not a str
            ValueError: If do_ek_scores not a bool
            ValueError: If report_dir not a str
        &#34;&#34;&#34;
        self.training_data_file = training_data_file
        self.test_data_file = test_data_file
        self.do_ek_scores = do_ek_scores
        self.report_dir = report_dir
        self.tests = tests
        self.cv_fold = cv_fold
        self.n_vals = n_vals
        self.nu_vals = nu_vals
        self.iters = iters
        self.test_name = test_name
        self.random_state = None


        if params is not None:
            for key, val in params.items():
                match key:
                    case &#34;tests&#34;:
                        if type(val) is int and val &gt;= 1:
                            self.tests = val
                        else:
                            raise ValueError(&#34;number of tests must be an int &gt;=1&#34;)
                    case &#34;cv_fold&#34;:
                        if type(val) is int and val &gt;= 2:
                            self.cv_fold = val
                        else:
                            raise ValueError(&#34;number of cv folds must be an int &gt;=2&#34;)
                    case &#34;iters&#34;:
                        if all(elem &gt;= 1 for elem in val):
                            self.iters = val
                        else:
                            raise ValueError(
                                &#34;iters must be positve int &gt;= 1, default is 5000&#34;
                            )
                    case &#34;n_vals&#34;:
                        if all(elem &gt;= 1 and isinstance(elem, int) for elem in val):
                            self.n_vals = val
                        else:
                            raise ValueError(
                                &#34;n_vals must be a list of positve int &gt;= 1, default is 6000&#34;
                            )
                    case &#34;nu_vals&#34;:
                        if all(elem &gt;= 1 and isinstance(elem, int) for elem in val):
                            self.nu_vals = val
                        else:
                            raise ValueError(
                                &#34;nu_vals must be a list of positve int &gt;= 1, default is 5&#34;
                            )
                    case &#34;training_data_file&#34;:
                        if val is not None:
                            if os.path.isfile(pathlib.Path(&#39;data&#39;,val)):
                                self.training_data_file = val
                            else:
                                raise FileNotFoundError(f&#34;file not found at {str(pathlib.Path(&#39;data&#39;,val))}&#34;)
                            
                        else:
                            raise ValueError(&#34;training_data_file must be str and a file in the ./data directory&#34;)
                        
                    case &#34;test_data_file&#34;:
                        #assert (type(val) is str, &#34;test_data_file value must be a str&#34;)
                        if val is not None:
                            if os.path.isfile(pathlib.Path(&#39;data&#39;,val)):
                                self.test_data_file = val
                            else:
                                raise FileNotFoundError(f&#34;file not found at {str(pathlib.Path(&#39;data&#39;,val))}&#34;)
                        else:
                            raise ValueError(&#34;test_data_file must be str and a file in the ./data directory&#34;)
                        
                    case &#34;class_label&#34;:
                        if val is not None and isinstance(val, str):
                            self.class_label = val
                        else:
                            raise ValueError(&#34;class_label must be str&#34;)
                        
                    case &#34;test_name&#34;:
                        if val is not None and isinstance(val,str):
                            self.test_name = val
                        else:
                            raise ValueError(&#34;test_name must be str&#34;)
                        
                    case &#34;do_ek_scores&#34;:
                        if val is not None and isinstance(val,bool):
                            self.do_ek_scores = val
                        else:
                            raise ValueError(&#34;do_ek_scores must be bool&#34;)
                        
                    case &#34;random_state&#34;:
                        if val is not None and isinstance(val, int) and val &gt;= 0:
                            self.random_state = val
                        else:
                            raise ValueError(&#34;number of tests must be an int &gt;=1&#34;)
                        
                    case &#34;rules&#34;:
                        if val is not None and isinstance(val,str):
                            self.rules_dir = val
                        else:
                            raise ValueError(&#34;rules directory must be a valid string&#34;)
                    case _:
                        print(f&#34;param: {key} not supported.&#34;)

        try:
            
            if self.test_name is not None and self.class_label is not None and self.rules_dir is not None:
                # create DTI object for this test instance.
                self.dti = DTILCS(self.test_name, class_label=self.class_label, rules=self.rules_dir)
            else:
                raise ValueError(&#34;missing argument for DTILCS object, be sure name, label, and rules are provided&#34;)
        except RuntimeError as e:
            raise RuntimeError(&#34;failed to create dti object to start tests&#34;) from e    

        
    def start(self):
        &#34;&#34;&#34;Start trial using current dit, test data, training data and config settings.

        Raises:
            IOError: If report directory can not be created.
            ValueError: Not all test configuration values are set.
            RuntimeError: If model fails to fit.
        &#34;&#34;&#34;
        try:
            if not os.path.exists(self.report_dir):
                os.makedirs(self.report_dir)
                
        except IOError as e:
            raise IOError(&#34;failed to create report directory.&#34;) from e

        if any(
            elem is None
            for elem in list([self.dti, self.test_data_file, self.training_data_file])
        ):
            raise ValueError(&#34;test configuration values not all set&#34;)

        print(f&#34;Running {str(self.tests)} test per setting level.&#34;)
        for itr in self.iters:
            for n in self.n_vals:
                for nu in self.nu_vals:
                    self.cv = []
                    self.accuracy = []
                    self.bal_accuracy = []
                    self.error = []
                    self.true_neg = []
                    self.false_pos = []
                    self.false_neg = []
                    self.true_pos = []
                    self.total_obs = []
                    self.fit_times = []
                    self.mccs = []
                    self.f1s = []

                    for i in range(self.tests):
                        try:
                            print(
                                f&#34;\nTrial : {str(i)+&#39;_&#39;+str(itr)+&#39;_&#39;+str(n)+&#39;_&#39;+str(nu)}, with iter:{str(itr)}, N:{str(n)}, nu:{str(nu)}&#34;
                            )
                            self.dti = DTILCS(self.dti.name, class_label=self.class_label)
                            self.dti.load_train_data(self.training_data_file)
                            self.dti.load_test_data(self.test_data_file)
                            #self.x_trn = X_over
                            #self.y_trn = y_over
                            self.dti.prep_dataset(
                                do_ek=self.do_ek_scores,
                                random_state=self.random_state,
                            )
                            self.dti.compile(trial=i, iters=itr, N=n, nu=nu)
                            
                        except RuntimeError as e:
                            raise RuntimeError(&#34;Error while setting up training and test data for trail &#34;) from e
                        
                        try:
                            with Timer() as t:
                                try:
                                    self.dti.fit()
                                    self.dti.save_model()
                                    #self.dti.do_analysis()
                                except RuntimeError as e:
                                    raise RuntimeError (&#34;Failed to fit model check configuraiton.&#34;) from e
                        finally:
                            self.fit_times.append(round(t.interval, 2))

                        #self.cv.append(self.dti.cross_validate(k=self.cv_fold))
                        #CV turned off!
                        self.cv.append(0)

                        acc, bal_acc, err, cm = self.dti.test_model(
                            train_data=self.training_data_file,
                            class_label=self.class_label,
                        )

                        tn, fp, fn, tp = cm.ravel()
                        self.total_obs.append(np.sum((tn, fp, fn, tp)))
                        self.true_neg.append(tn)
                        self.false_pos.append(fp)
                        self.false_neg.append(fn)
                        self.true_pos.append(tp)
                        self.accuracy.append(acc)
                        self.bal_accuracy.append(bal_acc)
                        self.error.append(err)
                        N = tn + tp + fn + fp
                        S = (tp + fn) / N
                        P = (tp + fp) / N
                        _mcc= ((tp/N) - (S*P)) / math.sqrt(P*S*(1-S)*(1-P))
                        rec = tp / (tp+fp)
                        prc = tp / (tp+fn)
                        _f1 = 2 * prc * rec / (prc + rec)
                        self.mccs.append(_mcc)
                        self.f1s.append(_f1)

                    rpt_name = pathlib.Path(
                        &#34;./reports&#34;,
                        f&#39;{self.test_name}_{str(self.tests)}_tests_{&#34;iter-&#34; + str(itr) + &#34;_nu-&#34; + str(n) + &#34;_n-&#34; + str(nu)}.csv&#39;,
                    )
                    with open(rpt_name, &#34;w&#34;, encoding=&#34;utf-8&#34;) as report:
                        report.write(f&#34;iter,N,nu,accuracy,bal_accuracy,error,cv{str(self.cv_fold)},true_neg,total_obs,false_pos,false_neg,true_pos,fit_time,mcc,f1\n&#34;)
                        for i in range(self.tests):
                            report.write(
                                   f&#34;{str(itr)},\
                                    {str(n)},\
                                    {str(nu)},\
                                    {str(self.accuracy[i])},\
                                    {str(self.bal_accuracy[i])},\
                                    {str(self.error[i])},\
                                    {str(self.cv[i])},\
                                    {str(self.total_obs[i])},\
                                    {str(self.true_neg[i])},\
                                    {str(self.false_pos[i])},\
                                    {str(self.false_neg[i])},\
                                    {str(self.true_pos[i])},\
                                    {str(self.fit_times[i])},\
                                    {str(self.mccs[i])},\
                                    {str(self.f1s[i])}\n&#34;)
                                
                    sum_rpt_name = pathlib.Path(
                        &#34;./reports&#34;,
                        f&#39;{self.test_name}_{str(self.tests)}_tests_{&#34;iter-&#34; + str(itr) + &#34;_nu-&#34; + str(n) + &#34;_n-&#34; + str(nu)}_summary.csv&#39;,
                    )
                    with open(sum_rpt_name, &#34;w&#34;, encoding=&#34;utf-8&#34;) as sum_report:
                        sum_report.write(
                            f&#34;iter,N,nu,accuracy,bal_accuracy,error,cv{str(self.cv_fold)},true_neg,total_obs,false_pos,false_neg,true_pos,fit_time,mcc,f1\n&#34;
                        )
                        sum_report.write(
                            f&#34;{str(itr)},\
                              {str(n)},\
                              {str(nu)},\
                              {str(np.mean(self.accuracy))},\
                              {str(np.mean(self.bal_accuracy))},\
                              {str(np.mean(self.error))},\
                              {str(round(np.mean(self.cv), 4))},\
                              {str(np.mean(self.total_obs))},\
                              {str(np.mean(self.true_neg))},\
                              {str(np.mean(self.false_pos))},\
                              {str(np.mean(self.false_neg))},\
                              {str(np.mean(self.true_pos))},\
                              {str(round(np.mean(self.fit_times), 4))},\
                              {str(round(np.mean(self.mccs), 4))},\
                              {str(round(np.mean(self.f1s), 4))}&#34;
                        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="app.dti.proto.DTILCSTest.DTILCSTest"><code class="flex name class">
<span>class <span class="ident">DTILCSTest</span></span>
</code></dt>
<dd>
<div class="desc"><p>Digital Trace Inspector Learning Classifier System Test Suite is a custom test parameter testings suite to provide
an interface for users of DTILCS to build many models and adjust hyperparameters. This suite also creats reports
showing results from each trial.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DTILCSTest:
    &#34;&#34;&#34;Digital Trace Inspector Learning Classifier System Test Suite is a custom test parameter testings suite to provide
    an interface for users of DTILCS to build many models and adjust hyperparameters. This suite also creats reports 
    showing results from each trial.
    &#34;&#34;&#34;
    def __int__(self, iters=[10000], n_vals=[6000], nu_vals=[5])-&gt;None:
        self.dti = None
        self.tests = 5
        self.cv_fold = 3
        self.n_vals = n_vals
        self.nu_vals = nu_vals
        self.iters = iters
        self.training_data_file = None
        self.test_data_file = None
        self.cv = []
        self.accuracy = []
        self.bal_accuracy = []
        self.error = []
        self.true_neg = []
        self.false_pos = []
        self.false_neg = []
        self.true_pos = []
        self.total_obs = []
        self.fit_times = []
        self.class_label = None
        self.rules_dir = None

    def compile(
        self,
        training_data_file=None,
        test_data_file=None,
        test_name=None,
        do_ek_scores=False,
        report_dir=&#34;./reports/&#34;,
        tests=10,
        cv_fold=3,
        iters=[10000],
        n_vals=[6000],
        nu_vals=[5],
        params=None,
    ):
        &#34;&#34;&#34;Compile test settings before executing.

        Args:
            training_data_file (_type_, optional): Feature vector used to train the model. Defaults to None.
            test_data_file (_type_, optional): Feature vector to test. Defaults to None.
            class_label (str, optional): Class label used in feature vector. Defaults to &#34;relevant&#34;.
            test_name (str, optional): Title of the test for saving results. Defaults to &#34;Test&#34;.
            do_ek_scores (bool, optional): Fit option, if true use skrebate.ReliefF to calculate feature importance and speedup training. Defaults to False.
            report_dir (str, optional): Output directory for results. Defaults to &#34;./reports/&#34;.
            tests (int, optional): Number of tests to run at each setting level. Defaults to 10.
            cv_fold (int, optional): Cross validation fold value. Defaults to 3.
            iters (list, optional): Training iterations (epochs) through training dataset. Defaults to [10000].
            n_vals (list, optional): N-value hyper parameter -- maximum micro classifier population size (sum of classifier numerosities). Defaults to [6000].
            nu_vals (list, optional): Power parameter used to determine the importance of high accuracy when calculating fitness. Defaults to [5].
            params (_type_, optional): Configuration dict to configure model with a batch of settings. Defaults to None.

        Raises:
            ValueError: If number of tests not an int &gt;= 1
            ValueError: If number of cv folds not an int &gt;= 2
            ValueError: If number of iters not positve int &gt;= 1
            ValueError: If number of n_vals not positve int &gt;= 1
            ValueError: If nu_vals not positve int &gt;= 1
            ValueError: If training_data_file not a str
            ValueError: If test_data_file not a str
            ValueError: If test_name value not a str
            ValueError: If test_name not a str
            ValueError: If do_ek_scores not a bool
            ValueError: If report_dir not a str
        &#34;&#34;&#34;
        self.training_data_file = training_data_file
        self.test_data_file = test_data_file
        self.do_ek_scores = do_ek_scores
        self.report_dir = report_dir
        self.tests = tests
        self.cv_fold = cv_fold
        self.n_vals = n_vals
        self.nu_vals = nu_vals
        self.iters = iters
        self.test_name = test_name
        self.random_state = None


        if params is not None:
            for key, val in params.items():
                match key:
                    case &#34;tests&#34;:
                        if type(val) is int and val &gt;= 1:
                            self.tests = val
                        else:
                            raise ValueError(&#34;number of tests must be an int &gt;=1&#34;)
                    case &#34;cv_fold&#34;:
                        if type(val) is int and val &gt;= 2:
                            self.cv_fold = val
                        else:
                            raise ValueError(&#34;number of cv folds must be an int &gt;=2&#34;)
                    case &#34;iters&#34;:
                        if all(elem &gt;= 1 for elem in val):
                            self.iters = val
                        else:
                            raise ValueError(
                                &#34;iters must be positve int &gt;= 1, default is 5000&#34;
                            )
                    case &#34;n_vals&#34;:
                        if all(elem &gt;= 1 and isinstance(elem, int) for elem in val):
                            self.n_vals = val
                        else:
                            raise ValueError(
                                &#34;n_vals must be a list of positve int &gt;= 1, default is 6000&#34;
                            )
                    case &#34;nu_vals&#34;:
                        if all(elem &gt;= 1 and isinstance(elem, int) for elem in val):
                            self.nu_vals = val
                        else:
                            raise ValueError(
                                &#34;nu_vals must be a list of positve int &gt;= 1, default is 5&#34;
                            )
                    case &#34;training_data_file&#34;:
                        if val is not None:
                            if os.path.isfile(pathlib.Path(&#39;data&#39;,val)):
                                self.training_data_file = val
                            else:
                                raise FileNotFoundError(f&#34;file not found at {str(pathlib.Path(&#39;data&#39;,val))}&#34;)
                            
                        else:
                            raise ValueError(&#34;training_data_file must be str and a file in the ./data directory&#34;)
                        
                    case &#34;test_data_file&#34;:
                        #assert (type(val) is str, &#34;test_data_file value must be a str&#34;)
                        if val is not None:
                            if os.path.isfile(pathlib.Path(&#39;data&#39;,val)):
                                self.test_data_file = val
                            else:
                                raise FileNotFoundError(f&#34;file not found at {str(pathlib.Path(&#39;data&#39;,val))}&#34;)
                        else:
                            raise ValueError(&#34;test_data_file must be str and a file in the ./data directory&#34;)
                        
                    case &#34;class_label&#34;:
                        if val is not None and isinstance(val, str):
                            self.class_label = val
                        else:
                            raise ValueError(&#34;class_label must be str&#34;)
                        
                    case &#34;test_name&#34;:
                        if val is not None and isinstance(val,str):
                            self.test_name = val
                        else:
                            raise ValueError(&#34;test_name must be str&#34;)
                        
                    case &#34;do_ek_scores&#34;:
                        if val is not None and isinstance(val,bool):
                            self.do_ek_scores = val
                        else:
                            raise ValueError(&#34;do_ek_scores must be bool&#34;)
                        
                    case &#34;random_state&#34;:
                        if val is not None and isinstance(val, int) and val &gt;= 0:
                            self.random_state = val
                        else:
                            raise ValueError(&#34;number of tests must be an int &gt;=1&#34;)
                        
                    case &#34;rules&#34;:
                        if val is not None and isinstance(val,str):
                            self.rules_dir = val
                        else:
                            raise ValueError(&#34;rules directory must be a valid string&#34;)
                    case _:
                        print(f&#34;param: {key} not supported.&#34;)

        try:
            
            if self.test_name is not None and self.class_label is not None and self.rules_dir is not None:
                # create DTI object for this test instance.
                self.dti = DTILCS(self.test_name, class_label=self.class_label, rules=self.rules_dir)
            else:
                raise ValueError(&#34;missing argument for DTILCS object, be sure name, label, and rules are provided&#34;)
        except RuntimeError as e:
            raise RuntimeError(&#34;failed to create dti object to start tests&#34;) from e    

        
    def start(self):
        &#34;&#34;&#34;Start trial using current dit, test data, training data and config settings.

        Raises:
            IOError: If report directory can not be created.
            ValueError: Not all test configuration values are set.
            RuntimeError: If model fails to fit.
        &#34;&#34;&#34;
        try:
            if not os.path.exists(self.report_dir):
                os.makedirs(self.report_dir)
                
        except IOError as e:
            raise IOError(&#34;failed to create report directory.&#34;) from e

        if any(
            elem is None
            for elem in list([self.dti, self.test_data_file, self.training_data_file])
        ):
            raise ValueError(&#34;test configuration values not all set&#34;)

        print(f&#34;Running {str(self.tests)} test per setting level.&#34;)
        for itr in self.iters:
            for n in self.n_vals:
                for nu in self.nu_vals:
                    self.cv = []
                    self.accuracy = []
                    self.bal_accuracy = []
                    self.error = []
                    self.true_neg = []
                    self.false_pos = []
                    self.false_neg = []
                    self.true_pos = []
                    self.total_obs = []
                    self.fit_times = []
                    self.mccs = []
                    self.f1s = []

                    for i in range(self.tests):
                        try:
                            print(
                                f&#34;\nTrial : {str(i)+&#39;_&#39;+str(itr)+&#39;_&#39;+str(n)+&#39;_&#39;+str(nu)}, with iter:{str(itr)}, N:{str(n)}, nu:{str(nu)}&#34;
                            )
                            self.dti = DTILCS(self.dti.name, class_label=self.class_label)
                            self.dti.load_train_data(self.training_data_file)
                            self.dti.load_test_data(self.test_data_file)
                            #self.x_trn = X_over
                            #self.y_trn = y_over
                            self.dti.prep_dataset(
                                do_ek=self.do_ek_scores,
                                random_state=self.random_state,
                            )
                            self.dti.compile(trial=i, iters=itr, N=n, nu=nu)
                            
                        except RuntimeError as e:
                            raise RuntimeError(&#34;Error while setting up training and test data for trail &#34;) from e
                        
                        try:
                            with Timer() as t:
                                try:
                                    self.dti.fit()
                                    self.dti.save_model()
                                    #self.dti.do_analysis()
                                except RuntimeError as e:
                                    raise RuntimeError (&#34;Failed to fit model check configuraiton.&#34;) from e
                        finally:
                            self.fit_times.append(round(t.interval, 2))

                        #self.cv.append(self.dti.cross_validate(k=self.cv_fold))
                        #CV turned off!
                        self.cv.append(0)

                        acc, bal_acc, err, cm = self.dti.test_model(
                            train_data=self.training_data_file,
                            class_label=self.class_label,
                        )

                        tn, fp, fn, tp = cm.ravel()
                        self.total_obs.append(np.sum((tn, fp, fn, tp)))
                        self.true_neg.append(tn)
                        self.false_pos.append(fp)
                        self.false_neg.append(fn)
                        self.true_pos.append(tp)
                        self.accuracy.append(acc)
                        self.bal_accuracy.append(bal_acc)
                        self.error.append(err)
                        N = tn + tp + fn + fp
                        S = (tp + fn) / N
                        P = (tp + fp) / N
                        _mcc= ((tp/N) - (S*P)) / math.sqrt(P*S*(1-S)*(1-P))
                        rec = tp / (tp+fp)
                        prc = tp / (tp+fn)
                        _f1 = 2 * prc * rec / (prc + rec)
                        self.mccs.append(_mcc)
                        self.f1s.append(_f1)

                    rpt_name = pathlib.Path(
                        &#34;./reports&#34;,
                        f&#39;{self.test_name}_{str(self.tests)}_tests_{&#34;iter-&#34; + str(itr) + &#34;_nu-&#34; + str(n) + &#34;_n-&#34; + str(nu)}.csv&#39;,
                    )
                    with open(rpt_name, &#34;w&#34;, encoding=&#34;utf-8&#34;) as report:
                        report.write(f&#34;iter,N,nu,accuracy,bal_accuracy,error,cv{str(self.cv_fold)},true_neg,total_obs,false_pos,false_neg,true_pos,fit_time,mcc,f1\n&#34;)
                        for i in range(self.tests):
                            report.write(
                                   f&#34;{str(itr)},\
                                    {str(n)},\
                                    {str(nu)},\
                                    {str(self.accuracy[i])},\
                                    {str(self.bal_accuracy[i])},\
                                    {str(self.error[i])},\
                                    {str(self.cv[i])},\
                                    {str(self.total_obs[i])},\
                                    {str(self.true_neg[i])},\
                                    {str(self.false_pos[i])},\
                                    {str(self.false_neg[i])},\
                                    {str(self.true_pos[i])},\
                                    {str(self.fit_times[i])},\
                                    {str(self.mccs[i])},\
                                    {str(self.f1s[i])}\n&#34;)
                                
                    sum_rpt_name = pathlib.Path(
                        &#34;./reports&#34;,
                        f&#39;{self.test_name}_{str(self.tests)}_tests_{&#34;iter-&#34; + str(itr) + &#34;_nu-&#34; + str(n) + &#34;_n-&#34; + str(nu)}_summary.csv&#39;,
                    )
                    with open(sum_rpt_name, &#34;w&#34;, encoding=&#34;utf-8&#34;) as sum_report:
                        sum_report.write(
                            f&#34;iter,N,nu,accuracy,bal_accuracy,error,cv{str(self.cv_fold)},true_neg,total_obs,false_pos,false_neg,true_pos,fit_time,mcc,f1\n&#34;
                        )
                        sum_report.write(
                            f&#34;{str(itr)},\
                              {str(n)},\
                              {str(nu)},\
                              {str(np.mean(self.accuracy))},\
                              {str(np.mean(self.bal_accuracy))},\
                              {str(np.mean(self.error))},\
                              {str(round(np.mean(self.cv), 4))},\
                              {str(np.mean(self.total_obs))},\
                              {str(np.mean(self.true_neg))},\
                              {str(np.mean(self.false_pos))},\
                              {str(np.mean(self.false_neg))},\
                              {str(np.mean(self.true_pos))},\
                              {str(round(np.mean(self.fit_times), 4))},\
                              {str(round(np.mean(self.mccs), 4))},\
                              {str(round(np.mean(self.f1s), 4))}&#34;
                        )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="app.dti.proto.DTILCSTest.DTILCSTest.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, training_data_file=None, test_data_file=None, test_name=None, do_ek_scores=False, report_dir='./reports/', tests=10, cv_fold=3, iters=[10000], n_vals=[6000], nu_vals=[5], params=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compile test settings before executing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>training_data_file</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>Feature vector used to train the model. Defaults to None.</dd>
<dt><strong><code>test_data_file</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>Feature vector to test. Defaults to None.</dd>
<dt><strong><code>class_label</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Class label used in feature vector. Defaults to "relevant".</dd>
<dt><strong><code>test_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the test for saving results. Defaults to "Test".</dd>
<dt><strong><code>do_ek_scores</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Fit option, if true use skrebate.ReliefF to calculate feature importance and speedup training. Defaults to False.</dd>
<dt><strong><code>report_dir</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Output directory for results. Defaults to "./reports/".</dd>
<dt><strong><code>tests</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of tests to run at each setting level. Defaults to 10.</dd>
<dt><strong><code>cv_fold</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Cross validation fold value. Defaults to 3.</dd>
<dt><strong><code>iters</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Training iterations (epochs) through training dataset. Defaults to [10000].</dd>
<dt><strong><code>n_vals</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>N-value hyper parameter &ndash; maximum micro classifier population size (sum of classifier numerosities). Defaults to [6000].</dd>
<dt><strong><code>nu_vals</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Power parameter used to determine the importance of high accuracy when calculating fitness. Defaults to [5].</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>Configuration dict to configure model with a batch of settings. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If number of tests not an int &gt;= 1</dd>
<dt><code>ValueError</code></dt>
<dd>If number of cv folds not an int &gt;= 2</dd>
<dt><code>ValueError</code></dt>
<dd>If number of iters not positve int &gt;= 1</dd>
<dt><code>ValueError</code></dt>
<dd>If number of n_vals not positve int &gt;= 1</dd>
<dt><code>ValueError</code></dt>
<dd>If nu_vals not positve int &gt;= 1</dd>
<dt><code>ValueError</code></dt>
<dd>If training_data_file not a str</dd>
<dt><code>ValueError</code></dt>
<dd>If test_data_file not a str</dd>
<dt><code>ValueError</code></dt>
<dd>If test_name value not a str</dd>
<dt><code>ValueError</code></dt>
<dd>If test_name not a str</dd>
<dt><code>ValueError</code></dt>
<dd>If do_ek_scores not a bool</dd>
<dt><code>ValueError</code></dt>
<dd>If report_dir not a str</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(
    self,
    training_data_file=None,
    test_data_file=None,
    test_name=None,
    do_ek_scores=False,
    report_dir=&#34;./reports/&#34;,
    tests=10,
    cv_fold=3,
    iters=[10000],
    n_vals=[6000],
    nu_vals=[5],
    params=None,
):
    &#34;&#34;&#34;Compile test settings before executing.

    Args:
        training_data_file (_type_, optional): Feature vector used to train the model. Defaults to None.
        test_data_file (_type_, optional): Feature vector to test. Defaults to None.
        class_label (str, optional): Class label used in feature vector. Defaults to &#34;relevant&#34;.
        test_name (str, optional): Title of the test for saving results. Defaults to &#34;Test&#34;.
        do_ek_scores (bool, optional): Fit option, if true use skrebate.ReliefF to calculate feature importance and speedup training. Defaults to False.
        report_dir (str, optional): Output directory for results. Defaults to &#34;./reports/&#34;.
        tests (int, optional): Number of tests to run at each setting level. Defaults to 10.
        cv_fold (int, optional): Cross validation fold value. Defaults to 3.
        iters (list, optional): Training iterations (epochs) through training dataset. Defaults to [10000].
        n_vals (list, optional): N-value hyper parameter -- maximum micro classifier population size (sum of classifier numerosities). Defaults to [6000].
        nu_vals (list, optional): Power parameter used to determine the importance of high accuracy when calculating fitness. Defaults to [5].
        params (_type_, optional): Configuration dict to configure model with a batch of settings. Defaults to None.

    Raises:
        ValueError: If number of tests not an int &gt;= 1
        ValueError: If number of cv folds not an int &gt;= 2
        ValueError: If number of iters not positve int &gt;= 1
        ValueError: If number of n_vals not positve int &gt;= 1
        ValueError: If nu_vals not positve int &gt;= 1
        ValueError: If training_data_file not a str
        ValueError: If test_data_file not a str
        ValueError: If test_name value not a str
        ValueError: If test_name not a str
        ValueError: If do_ek_scores not a bool
        ValueError: If report_dir not a str
    &#34;&#34;&#34;
    self.training_data_file = training_data_file
    self.test_data_file = test_data_file
    self.do_ek_scores = do_ek_scores
    self.report_dir = report_dir
    self.tests = tests
    self.cv_fold = cv_fold
    self.n_vals = n_vals
    self.nu_vals = nu_vals
    self.iters = iters
    self.test_name = test_name
    self.random_state = None


    if params is not None:
        for key, val in params.items():
            match key:
                case &#34;tests&#34;:
                    if type(val) is int and val &gt;= 1:
                        self.tests = val
                    else:
                        raise ValueError(&#34;number of tests must be an int &gt;=1&#34;)
                case &#34;cv_fold&#34;:
                    if type(val) is int and val &gt;= 2:
                        self.cv_fold = val
                    else:
                        raise ValueError(&#34;number of cv folds must be an int &gt;=2&#34;)
                case &#34;iters&#34;:
                    if all(elem &gt;= 1 for elem in val):
                        self.iters = val
                    else:
                        raise ValueError(
                            &#34;iters must be positve int &gt;= 1, default is 5000&#34;
                        )
                case &#34;n_vals&#34;:
                    if all(elem &gt;= 1 and isinstance(elem, int) for elem in val):
                        self.n_vals = val
                    else:
                        raise ValueError(
                            &#34;n_vals must be a list of positve int &gt;= 1, default is 6000&#34;
                        )
                case &#34;nu_vals&#34;:
                    if all(elem &gt;= 1 and isinstance(elem, int) for elem in val):
                        self.nu_vals = val
                    else:
                        raise ValueError(
                            &#34;nu_vals must be a list of positve int &gt;= 1, default is 5&#34;
                        )
                case &#34;training_data_file&#34;:
                    if val is not None:
                        if os.path.isfile(pathlib.Path(&#39;data&#39;,val)):
                            self.training_data_file = val
                        else:
                            raise FileNotFoundError(f&#34;file not found at {str(pathlib.Path(&#39;data&#39;,val))}&#34;)
                        
                    else:
                        raise ValueError(&#34;training_data_file must be str and a file in the ./data directory&#34;)
                    
                case &#34;test_data_file&#34;:
                    #assert (type(val) is str, &#34;test_data_file value must be a str&#34;)
                    if val is not None:
                        if os.path.isfile(pathlib.Path(&#39;data&#39;,val)):
                            self.test_data_file = val
                        else:
                            raise FileNotFoundError(f&#34;file not found at {str(pathlib.Path(&#39;data&#39;,val))}&#34;)
                    else:
                        raise ValueError(&#34;test_data_file must be str and a file in the ./data directory&#34;)
                    
                case &#34;class_label&#34;:
                    if val is not None and isinstance(val, str):
                        self.class_label = val
                    else:
                        raise ValueError(&#34;class_label must be str&#34;)
                    
                case &#34;test_name&#34;:
                    if val is not None and isinstance(val,str):
                        self.test_name = val
                    else:
                        raise ValueError(&#34;test_name must be str&#34;)
                    
                case &#34;do_ek_scores&#34;:
                    if val is not None and isinstance(val,bool):
                        self.do_ek_scores = val
                    else:
                        raise ValueError(&#34;do_ek_scores must be bool&#34;)
                    
                case &#34;random_state&#34;:
                    if val is not None and isinstance(val, int) and val &gt;= 0:
                        self.random_state = val
                    else:
                        raise ValueError(&#34;number of tests must be an int &gt;=1&#34;)
                    
                case &#34;rules&#34;:
                    if val is not None and isinstance(val,str):
                        self.rules_dir = val
                    else:
                        raise ValueError(&#34;rules directory must be a valid string&#34;)
                case _:
                    print(f&#34;param: {key} not supported.&#34;)

    try:
        
        if self.test_name is not None and self.class_label is not None and self.rules_dir is not None:
            # create DTI object for this test instance.
            self.dti = DTILCS(self.test_name, class_label=self.class_label, rules=self.rules_dir)
        else:
            raise ValueError(&#34;missing argument for DTILCS object, be sure name, label, and rules are provided&#34;)
    except RuntimeError as e:
        raise RuntimeError(&#34;failed to create dti object to start tests&#34;) from e    </code></pre>
</details>
</dd>
<dt id="app.dti.proto.DTILCSTest.DTILCSTest.start"><code class="name flex">
<span>def <span class="ident">start</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Start trial using current dit, test data, training data and config settings.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>IOError</code></dt>
<dd>If report directory can not be created.</dd>
<dt><code>ValueError</code></dt>
<dd>Not all test configuration values are set.</dd>
<dt><code>RuntimeError</code></dt>
<dd>If model fails to fit.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start(self):
    &#34;&#34;&#34;Start trial using current dit, test data, training data and config settings.

    Raises:
        IOError: If report directory can not be created.
        ValueError: Not all test configuration values are set.
        RuntimeError: If model fails to fit.
    &#34;&#34;&#34;
    try:
        if not os.path.exists(self.report_dir):
            os.makedirs(self.report_dir)
            
    except IOError as e:
        raise IOError(&#34;failed to create report directory.&#34;) from e

    if any(
        elem is None
        for elem in list([self.dti, self.test_data_file, self.training_data_file])
    ):
        raise ValueError(&#34;test configuration values not all set&#34;)

    print(f&#34;Running {str(self.tests)} test per setting level.&#34;)
    for itr in self.iters:
        for n in self.n_vals:
            for nu in self.nu_vals:
                self.cv = []
                self.accuracy = []
                self.bal_accuracy = []
                self.error = []
                self.true_neg = []
                self.false_pos = []
                self.false_neg = []
                self.true_pos = []
                self.total_obs = []
                self.fit_times = []
                self.mccs = []
                self.f1s = []

                for i in range(self.tests):
                    try:
                        print(
                            f&#34;\nTrial : {str(i)+&#39;_&#39;+str(itr)+&#39;_&#39;+str(n)+&#39;_&#39;+str(nu)}, with iter:{str(itr)}, N:{str(n)}, nu:{str(nu)}&#34;
                        )
                        self.dti = DTILCS(self.dti.name, class_label=self.class_label)
                        self.dti.load_train_data(self.training_data_file)
                        self.dti.load_test_data(self.test_data_file)
                        #self.x_trn = X_over
                        #self.y_trn = y_over
                        self.dti.prep_dataset(
                            do_ek=self.do_ek_scores,
                            random_state=self.random_state,
                        )
                        self.dti.compile(trial=i, iters=itr, N=n, nu=nu)
                        
                    except RuntimeError as e:
                        raise RuntimeError(&#34;Error while setting up training and test data for trail &#34;) from e
                    
                    try:
                        with Timer() as t:
                            try:
                                self.dti.fit()
                                self.dti.save_model()
                                #self.dti.do_analysis()
                            except RuntimeError as e:
                                raise RuntimeError (&#34;Failed to fit model check configuraiton.&#34;) from e
                    finally:
                        self.fit_times.append(round(t.interval, 2))

                    #self.cv.append(self.dti.cross_validate(k=self.cv_fold))
                    #CV turned off!
                    self.cv.append(0)

                    acc, bal_acc, err, cm = self.dti.test_model(
                        train_data=self.training_data_file,
                        class_label=self.class_label,
                    )

                    tn, fp, fn, tp = cm.ravel()
                    self.total_obs.append(np.sum((tn, fp, fn, tp)))
                    self.true_neg.append(tn)
                    self.false_pos.append(fp)
                    self.false_neg.append(fn)
                    self.true_pos.append(tp)
                    self.accuracy.append(acc)
                    self.bal_accuracy.append(bal_acc)
                    self.error.append(err)
                    N = tn + tp + fn + fp
                    S = (tp + fn) / N
                    P = (tp + fp) / N
                    _mcc= ((tp/N) - (S*P)) / math.sqrt(P*S*(1-S)*(1-P))
                    rec = tp / (tp+fp)
                    prc = tp / (tp+fn)
                    _f1 = 2 * prc * rec / (prc + rec)
                    self.mccs.append(_mcc)
                    self.f1s.append(_f1)

                rpt_name = pathlib.Path(
                    &#34;./reports&#34;,
                    f&#39;{self.test_name}_{str(self.tests)}_tests_{&#34;iter-&#34; + str(itr) + &#34;_nu-&#34; + str(n) + &#34;_n-&#34; + str(nu)}.csv&#39;,
                )
                with open(rpt_name, &#34;w&#34;, encoding=&#34;utf-8&#34;) as report:
                    report.write(f&#34;iter,N,nu,accuracy,bal_accuracy,error,cv{str(self.cv_fold)},true_neg,total_obs,false_pos,false_neg,true_pos,fit_time,mcc,f1\n&#34;)
                    for i in range(self.tests):
                        report.write(
                               f&#34;{str(itr)},\
                                {str(n)},\
                                {str(nu)},\
                                {str(self.accuracy[i])},\
                                {str(self.bal_accuracy[i])},\
                                {str(self.error[i])},\
                                {str(self.cv[i])},\
                                {str(self.total_obs[i])},\
                                {str(self.true_neg[i])},\
                                {str(self.false_pos[i])},\
                                {str(self.false_neg[i])},\
                                {str(self.true_pos[i])},\
                                {str(self.fit_times[i])},\
                                {str(self.mccs[i])},\
                                {str(self.f1s[i])}\n&#34;)
                            
                sum_rpt_name = pathlib.Path(
                    &#34;./reports&#34;,
                    f&#39;{self.test_name}_{str(self.tests)}_tests_{&#34;iter-&#34; + str(itr) + &#34;_nu-&#34; + str(n) + &#34;_n-&#34; + str(nu)}_summary.csv&#39;,
                )
                with open(sum_rpt_name, &#34;w&#34;, encoding=&#34;utf-8&#34;) as sum_report:
                    sum_report.write(
                        f&#34;iter,N,nu,accuracy,bal_accuracy,error,cv{str(self.cv_fold)},true_neg,total_obs,false_pos,false_neg,true_pos,fit_time,mcc,f1\n&#34;
                    )
                    sum_report.write(
                        f&#34;{str(itr)},\
                          {str(n)},\
                          {str(nu)},\
                          {str(np.mean(self.accuracy))},\
                          {str(np.mean(self.bal_accuracy))},\
                          {str(np.mean(self.error))},\
                          {str(round(np.mean(self.cv), 4))},\
                          {str(np.mean(self.total_obs))},\
                          {str(np.mean(self.true_neg))},\
                          {str(np.mean(self.false_pos))},\
                          {str(np.mean(self.false_neg))},\
                          {str(np.mean(self.true_pos))},\
                          {str(round(np.mean(self.fit_times), 4))},\
                          {str(round(np.mean(self.mccs), 4))},\
                          {str(round(np.mean(self.f1s), 4))}&#34;
                    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="app.dti.proto" href="index.html">app.dti.proto</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="app.dti.proto.DTILCSTest.DTILCSTest" href="#app.dti.proto.DTILCSTest.DTILCSTest">DTILCSTest</a></code></h4>
<ul class="">
<li><code><a title="app.dti.proto.DTILCSTest.DTILCSTest.compile" href="#app.dti.proto.DTILCSTest.DTILCSTest.compile">compile</a></code></li>
<li><code><a title="app.dti.proto.DTILCSTest.DTILCSTest.start" href="#app.dti.proto.DTILCSTest.DTILCSTest.start">start</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>